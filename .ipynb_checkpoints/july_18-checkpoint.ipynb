{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b448ba2f-128d-4e59-91e1-45648297358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09dff6a-70cc-44e7-b1b0-c3fa2b4a8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce3bfd4-5e1c-4430-b9c2-ac53a5b8bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "better | better\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'eats', 'ate', 'adjustable', 'rafting', 'ability', 'better']\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))\n",
    "\n",
    "# see how ate is still ate!\n",
    "# using stemmer can be efficient, it is faster, does not require lang knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8259fd4-ce3d-4bd4-be29-41e6af2cdc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meet\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "401fa04a-c26c-457d-8a17-db2851adf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali --> Ali\n",
      "literally --> literally\n",
      "talked --> talk\n",
      "for --> for\n",
      "three --> three\n",
      "damn --> damn\n",
      "hours --> hour\n",
      "about --> about\n",
      "nutrition --> nutrition\n",
      ", --> ,\n",
      "chickens --> chicken\n",
      ", --> ,\n",
      "and --> and\n",
      "shit --> shit\n",
      ". --> .\n",
      "Ai --> ai\n",
      "n't --> not\n",
      "nothing --> nothing\n",
      "else --> else\n",
      "! --> !\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Ali literally talked for three damn hours about nutrition, chickens, and shit. Ain't nothing else!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"-->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29bade16-4470-462d-a5b0-aced207c3158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro --> Bro\n",
      ", --> ,\n",
      "do --> do\n",
      "you --> you\n",
      "wanna --> wanna\n",
      "hang --> hang\n",
      "out --> out\n",
      "? --> ?\n",
      "No --> no\n",
      "brah --> brah\n",
      "I --> I\n",
      "'m --> be\n",
      "tired --> tired\n",
      "as --> as\n",
      "hell --> hell\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "# let's customize the lemmatization here!\n",
    "\n",
    "doc = nlp(\"Bro, do you wanna hang out? No brah I'm tired as hell.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"-->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "324f9b00-4133-48bd-96ae-e79845534a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro --> Brother\n",
      ", --> ,\n",
      "do --> do\n",
      "you --> you\n",
      "wanna --> wanna\n",
      "hang --> hang\n",
      "out --> out\n",
      "? --> ?\n",
      "No --> no\n",
      "brah --> brah\n",
      "I --> I\n",
      "'m --> be\n",
      "tired --> tired\n",
      "as --> as\n",
      "hell --> hell\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "ar.add([[{'TEXT':'Bro'}], [{'TEXT':'Brah'}]], {'LEMMA':'Brother' }) # here the magic happens, the customization!\n",
    "\n",
    "doc = nlp(\"Bro, do you wanna hang out? No brah I'm tired as hell.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"-->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "855994f7-44d9-4810-9e82-60d661f2dd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon ----> PROPN | proper noun\n",
      "is ----> AUX | auxiliary\n",
      "a ----> DET | determiner\n",
      "whore ----> NOUN | noun\n",
      "of ----> ADP | adposition\n",
      "Trump ----> PROPN | proper noun\n",
      "'s ----> PART | particle\n",
      ", ----> PUNCT | punctuation\n",
      "they ----> PRON | pronoun\n",
      "both ----> PRON | pronoun\n",
      "can ----> AUX | auxiliary\n",
      "suck ----> VERB | verb\n",
      "my ----> PRON | pronoun\n",
      "ass ----> NOUN | noun\n",
      ". ----> PUNCT | punctuation\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon is a whore of Trump's, they both can suck my ass.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, '---->', token.pos_, \"|\", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f22997f-448d-46c0-aa22-dda79815a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon ----> PROPN | proper noun | NNP noun, proper singular\n",
      "is ----> AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "a ----> DET | determiner | DT determiner\n",
      "whore ----> NOUN | noun | NN noun, singular or mass\n",
      "of ----> ADP | adposition | IN conjunction, subordinating or preposition\n",
      "Trump ----> PROPN | proper noun | NNP noun, proper singular\n",
      "'s ----> PART | particle | POS possessive ending\n",
      ", ----> PUNCT | punctuation | , punctuation mark, comma\n",
      "they ----> PRON | pronoun | PRP pronoun, personal\n",
      "both ----> PRON | pronoun | DT determiner\n",
      "can ----> AUX | auxiliary | MD verb, modal auxiliary\n",
      "suck ----> VERB | verb | VB verb, base form\n",
      "my ----> PRON | pronoun | PRP$ pronoun, possessive\n",
      "ass ----> NOUN | noun | NN noun, singular or mass\n",
      ". ----> PUNCT | punctuation | . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon is a whore of Trump's, they both can suck my ass.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, '---->', token.pos_, \"|\", spacy.explain(token.pos_), \"|\", token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8165df74-43da-46ae-a302-9a006ea32d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". ----> PUNCT | punctuation | . punctuation mark, sentence closer\n",
      ", ----> PUNCT | punctuation | , punctuation mark, comma\n",
      ". ----> PUNCT | punctuation | . punctuation mark, sentence closer\n",
      ", ----> PUNCT | punctuation | , punctuation mark, comma\n",
      ". ----> PUNCT | punctuation | . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "thg_text = \"The Hunger Games are a series of young adult dystopian novels written by American author Suzanne Collins. The series consists of a trilogy that follows teenage protagonist Katniss Everdeen, and two prequels. The Hunger Games universe is a dystopia set in Panem, a North American country consisting of the wealthy Capitol and 13 districts in varying states of poverty.\"\n",
    "\n",
    "doc = nlp(thg_text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ in ['SPACE', 'X', 'PUNCT']:\n",
    "        print(token, '---->', token.pos_, \"|\", spacy.explain(token.pos_), \"|\", token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdcf0aa-a72f-45ef-b69e-55ca9bf93c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[., ,, ., ,, .]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use the tagger to remove the punctuation marks?\n",
    "\n",
    "doc = nlp(thg_text)\n",
    "\n",
    "filtered_tokens = [] \n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ in ['SPACE', 'X', 'PUNCT']:\n",
    "        filtered_tokens.append(token) \n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8111b9-4b21-4d66-8341-bc2a6666583f",
   "metadata": {},
   "source": [
    "- NER is used with searching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949668d1-5c44-4347-9758-2d27f9b7f433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ddee9-da16-4815-a2d1-b59760fb1249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b41f-e61b-40f3-b33f-fbbd7094b0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47cf3f-83c3-43c5-b9e5-68408b59cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94025ad-9627-4863-8aa4-16990f6dd198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077e0a3-6711-4a7f-ae5b-bb18e45b43a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293aed-bb46-4805-afae-06ce35197806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a06dd-10e6-41ec-95f8-f14f19e63c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acbb60-6bda-4e57-864b-8f79eef20807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
